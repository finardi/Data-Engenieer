{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MovieLens - NM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1KQdpMlazM5VC8kAnqip6N_7iq9DWS6Le",
      "authorship_tag": "ABX9TyPr2yZkBDI/WOcjPX4y3EOQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/finardi/Data-Engenieer/blob/main/MovieLens_NM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpl5bgTU_pIr"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMMkJcQz_WH3"
      },
      "source": [
        "# basics\n",
        "import os\n",
        "import gc\n",
        "import math\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from time import time\n",
        "\n",
        "# scipy\n",
        "import scipy.sparse as sp\n",
        "\n",
        "# plots\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# torch\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# retrieval topK\n",
        "import heapq  "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYNAVTFH_odr",
        "outputId": "ee949178-11d2-41f8-cb91-dde3a8f6202d"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "manual_seed = 2357 # only primers ;)\n",
        " \n",
        "def deterministic(rep=True):\n",
        "    if rep:\n",
        "        np.random.seed(manual_seed)\n",
        "        torch.manual_seed(manual_seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed(manual_seed)\n",
        "            torch.cuda.manual_seed_all(manual_seed)\n",
        "        torch.backends.cudnn.enabled = False \n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        print(f'Experimento deterministico, seed: {manual_seed} -- ', end = '')\n",
        "        print(f'Existe {torch.cuda.device_count()} GPU\\\n",
        " {torch.cuda.get_device_name(0)} disponível.')\n",
        "    else:\n",
        "        print('Experimento randomico')\n",
        "deterministic()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Experimento deterministico, seed: 2357 -- Existe 1 GPU Tesla P100-PCIE-16GB disponível.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "By144Z5L_5ia"
      },
      "source": [
        "# Data prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAp1m1A2_5fp",
        "outputId": "73eab30d-271b-4c9f-8b40-48369f17b851"
      },
      "source": [
        "!unzip /content/drive/MyDrive/Colab\\ Notebooks/RecSys/movielens_datasets/ml100k/ml-100k.zip\n",
        "!mkdir Data\n",
        "\n",
        "INPUT_PATH = 'ml-100k/u.data'\n",
        "\n",
        "OUTPUT_PATH_TRAIN = 'Data/movielens.train.rating'\n",
        "OUTPUT_PATH_TEST = 'Data/movielens.test.rating'\n",
        "USER_FIELD = 'userID'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/Colab Notebooks/RecSys/movielens_datasets/ml100k/ml-100k.zip\n",
            "   creating: ml-100k/\n",
            "  inflating: ml-100k/allbut.pl       \n",
            "  inflating: ml-100k/mku.sh          \n",
            "  inflating: ml-100k/README          \n",
            "  inflating: ml-100k/u.data          \n",
            "  inflating: ml-100k/u.genre         \n",
            "  inflating: ml-100k/u.info          \n",
            "  inflating: ml-100k/u.item          \n",
            "  inflating: ml-100k/u.occupation    \n",
            "  inflating: ml-100k/u.user          \n",
            "  inflating: ml-100k/u1.base         \n",
            "  inflating: ml-100k/u1.test         \n",
            "  inflating: ml-100k/u2.base         \n",
            "  inflating: ml-100k/u2.test         \n",
            "  inflating: ml-100k/u3.base         \n",
            "  inflating: ml-100k/u3.test         \n",
            "  inflating: ml-100k/u4.base         \n",
            "  inflating: ml-100k/u4.test         \n",
            "  inflating: ml-100k/u5.base         \n",
            "  inflating: ml-100k/u5.test         \n",
            "  inflating: ml-100k/ua.base         \n",
            "  inflating: ml-100k/ua.test         \n",
            "  inflating: ml-100k/ub.base         \n",
            "  inflating: ml-100k/ub.test         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATFiUyQU_5dY",
        "outputId": "dd7ca66e-27f6-4b9f-c5c1-e006ea73ca9b"
      },
      "source": [
        "def save_to_csv(df,path, header=False, index=False, sep='\\t', verbose=False):\n",
        "    if verbose:\n",
        "        print(\"Saving df to path: {}\".format(path))\n",
        "        print(\"Columns in df are: {}\".format(df.columns.tolist()))\n",
        "\n",
        "    df.to_csv(path, header=header, index=index, sep=sep)\n",
        "\n",
        "def get_train_test_df(transactions):\n",
        "    print(\"Size of the entire dataset:{}\".format(transactions.shape))\n",
        "    transactions.sort_values(by=['timestamp'], inplace=True)\n",
        "    last_transaction_mask = transactions.duplicated(subset={USER_FIELD}, keep=\"last\")\n",
        "    train_df = transactions[last_transaction_mask]\n",
        "    test_df = transactions[~last_transaction_mask]\n",
        "    \n",
        "    train_df.sort_values(by=[\"userID\", 'timestamp'], inplace=True)\n",
        "    test_df.sort_values(by=[\"userID\", 'timestamp'], inplace=True)\n",
        "    return train_df, test_df\n",
        "\n",
        "def report_stats(transactions, train_df, test_df):\n",
        "    whole_size = transactions.shape[0]*1.0\n",
        "    train_size = train_df.shape[0]\n",
        "    test_size = test_df.shape[0]\n",
        "    print(\"Total No. of Records = {}\".format(whole_size))\n",
        "    print(\"Train size = {}, Test size = {}\".format(train_size, test_size))\n",
        "    print(\"Train % = {}, Test % ={}\".format(train_size/whole_size, test_size/whole_size))\n",
        "\n",
        "transactions = pd.read_csv(\n",
        "    INPUT_PATH, \n",
        "    sep=\"\\t\", \n",
        "    names=['userID', 'movieID', 'rating', 'timestamp'], \n",
        "    engine='python'\n",
        "    )\n",
        "\n",
        "transactions['rating'] = 1\n",
        "\n",
        "# --> make the dataset\n",
        "train_df, test_df = get_train_test_df(transactions)\n",
        "save_to_csv(train_df, OUTPUT_PATH_TRAIN, header=False, index=False, verbose=1)\n",
        "save_to_csv(test_df, OUTPUT_PATH_TEST, header=False, index=False, verbose=1)\n",
        "report_stats(transactions, train_df, test_df)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of the entire dataset:(100000, 4)\n",
            "Saving df to path: Data/movielens.train.rating\n",
            "Columns in df are: ['userID', 'movieID', 'rating', 'timestamp']\n",
            "Saving df to path: Data/movielens.test.rating\n",
            "Columns in df are: ['userID', 'movieID', 'rating', 'timestamp']\n",
            "Total No. of Records = 100000.0\n",
            "Train size = 99057, Test size = 943\n",
            "Train % = 0.99057, Test % =0.00943\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LUrXwT-2hj-"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QmWpCmu2mmJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eac71e3-b2fb-4851-c388-52f60e00903f"
      },
      "source": [
        "class RecSysDataset(Dataset):\n",
        "    def __init__(self, file_name, num_negatives_train=5, num_negatives_test=100):\n",
        "\n",
        "        self.trainMatrix = self.load_rating_file_as_matrix(file_name + \".train.rating\")\n",
        "        self.num_users, self.num_items = self.trainMatrix.shape\n",
        "        self.user_input, self.item_input, self.ratings = self.get_train_instances(self.trainMatrix, num_negatives_train)\n",
        "        self.testRatings = self.load_rating_file_as_list(file_name + \".test.rating\")\n",
        "        self.testNegatives = self.create_negative_file(num_samples=num_negatives_test)\n",
        "        \n",
        "        assert len(self.testRatings) == len(self.testNegatives)\n",
        "\n",
        "    def get_train_instances(self, train, num_negatives):\n",
        "        user_input, item_input, ratings = [], [], []\n",
        "        num_users, num_items = train.shape\n",
        "        for (u, i) in train.keys():\n",
        "            # positive instance\n",
        "            user_input.append(u)\n",
        "            item_input.append(i)\n",
        "            ratings.append(1)\n",
        "            # negative instances\n",
        "            for _ in range(num_negatives):\n",
        "                j = np.random.randint(1, num_items)\n",
        "                # while train.has_key((u, j)):\n",
        "                while (u, j) in train:\n",
        "                    j = np.random.randint(1, num_items)\n",
        "                user_input.append(u)\n",
        "                item_input.append(j)\n",
        "                ratings.append(0)\n",
        "        return user_input, item_input, ratings\n",
        "\n",
        "    def load_rating_file_as_list(self, filename):\n",
        "        ratingList = []\n",
        "        with open(filename, \"r\") as f:\n",
        "            line = f.readline()\n",
        "            while line != None and line != \"\":\n",
        "                arr = line.split(\"\\t\")\n",
        "                user, item = int(arr[0]), int(arr[1])\n",
        "                ratingList.append([user, item])\n",
        "                line = f.readline()\n",
        "        return ratingList\n",
        "\n",
        "    def create_negative_file(self, num_samples=100):\n",
        "        negativeList = []\n",
        "        for user_item_pair in self.testRatings:\n",
        "            user = user_item_pair[0]\n",
        "            item = user_item_pair[1]\n",
        "            negatives = []\n",
        "            for t in range(num_samples):\n",
        "                j = np.random.randint(1, self.num_items)\n",
        "                while (user, j) in self.trainMatrix or j == item:\n",
        "                    j = np.random.randint(1, self.num_items)\n",
        "                negatives.append(j)\n",
        "            negativeList.append(negatives)\n",
        "        return negativeList\n",
        "\n",
        "    def load_rating_file_as_matrix(self, filename):\n",
        "        num_users, num_items = 0, 0\n",
        "        with open(filename, \"r\") as f:\n",
        "            line = f.readline()\n",
        "            while line != None and line != \"\":\n",
        "                arr = line.split(\"\\t\")\n",
        "                u, i = int(arr[0]), int(arr[1])\n",
        "                num_users = max(num_users, u)\n",
        "                num_items = max(num_items, i)\n",
        "                line = f.readline()\n",
        "        \n",
        "        mat = sp.dok_matrix((num_users+1, num_items+1), dtype=np.float32)\n",
        "        with open(filename, \"r\") as f:\n",
        "            line = f.readline()\n",
        "            while line != None and line != \"\":\n",
        "                arr = line.split(\"\\t\")\n",
        "                user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
        "                if (rating > 0):\n",
        "                    mat[user, item] = 1.0\n",
        "                line = f.readline()\n",
        "        return mat\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.user_input)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        user_id = self.user_input[index]\n",
        "        item_id = self.item_input[index]\n",
        "        rating = self.ratings[index]\n",
        "\n",
        "        return {'user_id': user_id, 'item_id': item_id, 'rating': rating}\n",
        "\n",
        "# --> Testing RecSysDataset\n",
        "\n",
        "ds = RecSysDataset(\n",
        "    file_name='Data/movielens', \n",
        "    num_negatives_train=4,\n",
        "    num_negatives_test=100\n",
        "    )\n",
        "    \n",
        "ds_dict = ds[0]\n",
        "print('Testing RecSysDataset')\n",
        "print(f\"user_id: {ds_dict['user_id']}, item_id: {ds_dict['item_id']}, rating: {ds_dict['rating']}\")\n",
        "\n",
        "train, testRatings, testNegatives = ds.trainMatrix, ds.testRatings, ds.testNegatives\n",
        "num_users, num_items = train.shape\n",
        "\n",
        "print(f\"train: {train.shape}, testRatings: {len(testRatings)}, testNegatives: {len(testNegatives)}\")\n",
        "print(f\"num_users: {num_users}, num_items: {num_items}\")"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing RecSysDataset\n",
            "user_id: 1, item_id: 168, rating: 1\n",
            "train: (944, 1683), testRatings: 943, testNegatives: 943\n",
            "num_users: 944, num_items: 1683\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS0hMBDnHU4_"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RT_75AUeHXPy",
        "outputId": "c57b4685-0f03-4fad-e2b6-1813cedb4c23"
      },
      "source": [
        "BATCH_SZ = 256\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    ds, \n",
        "    batch_size=BATCH_SZ,\n",
        "    shuffle=True, \n",
        "    num_workers=os.cpu_count(), \n",
        "    pin_memory=True\n",
        "    )\n",
        "\n",
        "# --> Testing Dataloader\n",
        "dict_loader = next(iter(train_loader))\n",
        "print(f\"user_id: {dict_loader['user_id'].shape}, item_id: {dict_loader['item_id'].shape}, rating: {dict_loader['rating'].shape}\")\n",
        "\n",
        "print('len dataloader:', len(train_loader))"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "user_id: torch.Size([256]), item_id: torch.Size([256]), rating: torch.Size([256])\n",
            "len dataloader: 1935\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uMYzfAD3Ro0"
      },
      "source": [
        "# Train and Evaluate functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkecLzaGOw5E"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "re5z7PFGOwxl",
        "outputId": "fea4ffb4-5566-4f92-9696-b43ec0583e3a"
      },
      "source": [
        "class RecSysNet(torch.nn.Module):\n",
        "    def __init__(self, n_users, n_items,  embedding_dim=16, out=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.activation = torch.nn.ReLU()\n",
        "        self.drop = torch.nn.Dropout()\n",
        "        \n",
        "        embedding_dim = embedding_dim\n",
        "\n",
        "        self.user_embedding = torch.nn.Embedding(n_users, embedding_dim//2)\n",
        "        self.item_embedding = torch.nn.Embedding(n_items, embedding_dim//2)\n",
        "\n",
        "        self.fc_layer_0 = torch.nn.Linear(embedding_dim, 2 * embedding_dim)\n",
        "        self.ln_0 = torch.nn.LayerNorm(2 * embedding_dim)\n",
        "\n",
        "        self.fc_layer_1 = torch.nn.Linear(2 * embedding_dim, embedding_dim)\n",
        "        self.ln_1 = torch.nn.LayerNorm(embedding_dim)\n",
        "\n",
        "        self.fc_layer_2 = torch.nn.Linear(embedding_dim, embedding_dim//2)\n",
        "        self.ln_2 = torch.nn.LayerNorm(embedding_dim//2)\n",
        "\n",
        "        self.output_layer = torch.nn.Linear(embedding_dim//2, out)\n",
        "\n",
        "    def forward(self, feed_dict):\n",
        "        user_embedding = self.user_embedding(feed_dict['user_id'])\n",
        "        item_embedding = self.item_embedding(feed_dict['item_id'])\n",
        "\n",
        "        x = torch.cat([user_embedding, item_embedding], dim=1)\n",
        "\n",
        "        # o = self.drop(self.activation(self.fc_layer_0(x)))\n",
        "\n",
        "        o = self.activation(self.ln_0(self.fc_layer_0(x)))\n",
        "        o = self.activation(self.ln_1(self.fc_layer_1(o)))\n",
        "        o = self.activation(self.ln_2(self.fc_layer_2(o)))\n",
        "        \n",
        "        logit = self.output_layer(o)\n",
        "        rating = torch.sigmoid(logit)\n",
        "        return rating\n",
        "\n",
        "    def predict(self, feed_dict):\n",
        "        for key in feed_dict:\n",
        "            if type(feed_dict[key]) != type(None):\n",
        "                feed_dict[key] = torch.from_numpy(feed_dict[key]).to(dtype=torch.long, device=device)\n",
        "        output_scores = self.forward(feed_dict)\n",
        "        return output_scores.cpu().detach().numpy()\n",
        "\n",
        "# --> Testing the Network\n",
        "try:\n",
        "    del model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "model = RecSysNet(\n",
        "    n_users=num_users, \n",
        "    n_items=num_items, \n",
        ")    \n",
        "\n",
        "with torch.no_grad():\n",
        "    out = model(dict_loader)\n",
        "print(f'out forward shape: {out.shape}', end= '  --  ')\n",
        "print(f'out sample: {out[0]}')"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "out forward shape: torch.Size([256, 1])  --  out sample: tensor([0.6071])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1CmuG1IP2EM",
        "outputId": "0a5a6c7b-4b64-456b-c473-35bfaeb5d603"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "_ = 49 \n",
        "\n",
        "print('\\n','#' * _, f'\\n # Número de params. {count_parameters(model):,}' \\\n",
        "       ' trainable parameters #\\n', '#' * _,'\\n')  \n",
        "model"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " ################################################# \n",
            " # Número de params. 22,345 trainable parameters #\n",
            " ################################################# \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RecSysNet(\n",
              "  (activation): ReLU()\n",
              "  (drop): Dropout(p=0.5, inplace=False)\n",
              "  (user_embedding): Embedding(944, 8)\n",
              "  (item_embedding): Embedding(1683, 8)\n",
              "  (fc_layer_0): Linear(in_features=16, out_features=32, bias=True)\n",
              "  (ln_0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "  (fc_layer_1): Linear(in_features=32, out_features=16, bias=True)\n",
              "  (ln_1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
              "  (fc_layer_2): Linear(in_features=16, out_features=8, bias=True)\n",
              "  (ln_2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
              "  (output_layer): Linear(in_features=8, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSS1aiqH3Q9s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ee6ec2e-f5cd-4299-ff37-d5949409463b"
      },
      "source": [
        "def train_one_epoch(model, data_loader, loss_fn, optimizer, epoch_no, num_epoch, device, verbose=True):\n",
        "    print(f\"Epoch [{epoch_no}/{num_epoch}]: \", end= '')\n",
        "    epoch_loss = []\n",
        "    model.train()\n",
        "    for dict_train in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        for key in dict_train:\n",
        "            if type(dict_train[key]) != type(None):\n",
        "                dict_train[key] = dict_train[key].to(dtype=torch.long, device=device)\n",
        "\n",
        "        rating_prediction = model(dict_train)\n",
        "\n",
        "        rating_true = dict_train['rating']\n",
        "        rating_true = rating_true.float().view(rating_prediction.size())  \n",
        "        loss = loss_fn(rating_prediction, rating_true)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss.append(loss.item())\n",
        "\n",
        "    epoch_loss = np.mean(epoch_loss)\n",
        "    if verbose:\n",
        "        print(f\"Train Loss: {epoch_loss:.4}\")\n",
        "    return epoch_loss\n",
        "        \n",
        "def test(model, full_dataset: RecSysDataset, testRatings, topK):\n",
        "    model.eval()\n",
        "    hits, ndcgs, mrrs = evaluate_model(model, full_dataset, testRatings, topK)\n",
        "    hr = np.array(hits).mean()\n",
        "    ndcg = np.array(ndcgs).mean()\n",
        "    mrr = np.array(mrrs).mean()\n",
        "\n",
        "    print(f'              Eval: MRR = {mrr:.4} -- HR = {hr:.4} -- NDCG = {ndcg:.4}')\n",
        "    return hr, ndcg, mrr\n",
        "    \n",
        "def get_items_interacted(user_id, interaction_df):\n",
        "    userid_mask = interaction_df['userid'] == user_id\n",
        "    interacted_items = interaction_df.loc[userid_mask].courseid\n",
        "    return set(interacted_items if type(interacted_items) == pd.Series else [interacted_items])\n",
        "\n",
        "def evaluate_model(model, dataset: RecSysDataset, testRatings: list, topK: int):\n",
        "    testRatings = dataset.testRatings\n",
        "    testNegatives = dataset.testNegatives\n",
        "\n",
        "    mrrs, hits, ndcgs = [], [], []\n",
        "    for idx in range(len(testRatings)):\n",
        "        hr, ndcg, mrr = _eval_one_rating(\n",
        "            idx, \n",
        "            model, \n",
        "            dataset, \n",
        "            testRatings[idx], \n",
        "            testNegatives[idx],\n",
        "            topK\n",
        "            )\n",
        "        hits.append(hr)\n",
        "        ndcgs.append(ndcg)\n",
        "        mrrs.append(mrr)\n",
        "    return hits, ndcgs, mrrs\n",
        "\n",
        "def _eval_one_rating(idx, model, dataset, testRatings, testNegatives, topk=10):\n",
        "    rating = testRatings\n",
        "    items = testNegatives\n",
        "    u = rating[0]\n",
        "\n",
        "    gtItem = rating[1]\n",
        "    items.append(gtItem)\n",
        "    map_item_score = {}\n",
        "    users = np.full(len(items), u, dtype='int32')\n",
        "\n",
        "    feed_dict = {\n",
        "        'user_id': users,\n",
        "        'item_id': np.array(items),\n",
        "    }\n",
        "    \n",
        "    predictions = model.predict(feed_dict)\n",
        "    \n",
        "    for i in range(len(items)):\n",
        "        item = items[i]\n",
        "        map_item_score[item] = predictions[i]\n",
        "\n",
        "    ranklist = heapq.nlargest(topk, map_item_score, key=map_item_score.get)\n",
        "    \n",
        "    hr = getHitRatio(ranklist, gtItem)\n",
        "    ndcg = getNDCG(ranklist, gtItem)\n",
        "    mrr = getMRR(ranklist, gtItem)\n",
        "    \n",
        "    return hr, ndcg, mrr\n",
        "\n",
        "def getHitRatio(ranklist, gtItem):\n",
        "    for item in ranklist:\n",
        "        if item == gtItem:\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "def getNDCG(ranklist, gtItem):\n",
        "    for i in range(len(ranklist)):\n",
        "        item = ranklist[i]\n",
        "        if item == gtItem:\n",
        "            return math.log(2) / math.log(i+2)\n",
        "    return 0 \n",
        "\n",
        "def getMRR(ranklist, gtItem):\n",
        "    for i, pred in enumerate(ranklist):\n",
        "        if pred == gtItem:\n",
        "            return 1/(i+1)\n",
        "    else:\n",
        "        return 0       \n",
        "\n",
        "def plot_statistics(hr_list, ndcg_list, loss_list, path):\n",
        "    plt.figure()\n",
        "    hr = np.array(hr_list)\n",
        "    ndcg = np.array(ndcg_list)\n",
        "    loss = np.array(loss_list)\n",
        "    plt.plot(hr[:,0], hr[:,1],linestyle='-', marker='o', label = \"HR\")\n",
        "    plt.plot(ndcg[:,0], ndcg[:,1],linestyle='-', marker='v', label = \"NDCG\")\n",
        "    plt.plot(loss[:,0], loss[:,1],linestyle='-', marker='s', label = \"Loss\")\n",
        "\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.legend()    \n",
        "\n",
        "# --> Testing Init performance\n",
        "hr, ndcg, mrr = test(model.to(device), ds, testRatings, topK=10)"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              Eval: MRR = 0.1509 -- HR = 0.3955 -- NDCG = 0.2075\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef-hSw4ZbGpc"
      },
      "source": [
        "#### http://www.gabormelli.com/RKB/Mean_Reciprocal_Rank_(MRR)_Measure\n",
        "MRRs = []\n",
        "\n",
        "q1 = 3\n",
        "rank1 = [1,2,3]\n",
        "MRRs.append(getMRR(rank1, q1))\n",
        "\n",
        "q2 = 2\n",
        "rank2 = [1,2,3]\n",
        "MRRs.append(getMRR(rank2, q2))\n",
        "\n",
        "q3 = 1\n",
        "rank3 = [1,2,3]\n",
        "MRRs.append(getMRR(rank3, q3))\n",
        "\n",
        "print(MRRs)\n",
        ">>> [0.3333333333333333, 0.5, 1.0]\n",
        "\n",
        "np.array(MRRs).mean()\n",
        ">>> 0.611111111111111\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQ7ZjUsTE_mL",
        "outputId": "151bcd66-7b62-4410-d479-d9f3bef0e8b1"
      },
      "source": [
        "class RecSysNet(torch.nn.Module):\n",
        "    def __init__(self, n_users, n_items, embedding_dim=16, out=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.activation = torch.nn.ReLU()\n",
        "        self.drop = torch.nn.Dropout()\n",
        "        \n",
        "        embedding_dim = embedding_dim\n",
        "\n",
        "        self.user_embedding = torch.nn.Embedding(n_users, embedding_dim//2)\n",
        "        self.item_embedding = torch.nn.Embedding(n_items, embedding_dim//2)\n",
        "\n",
        "        self.fc_layer_0 = torch.nn.Linear(embedding_dim, 2 * embedding_dim)\n",
        "\n",
        "        self.fc_layer_1 = torch.nn.Linear(2 * embedding_dim, embedding_dim//2)\n",
        "\n",
        "        # self.fc_layer_2 = torch.nn.Linear(embedding_dim, embedding_dim//2)\n",
        "\n",
        "        self.output_layer = torch.nn.Linear(embedding_dim//2, out)\n",
        "\n",
        "    def forward(self, feed_dict):\n",
        "        user_embedding = self.user_embedding(feed_dict['user_id'])\n",
        "        item_embedding = self.item_embedding(feed_dict['item_id'])\n",
        "\n",
        "        x = torch.cat([user_embedding, item_embedding], dim=1)\n",
        "\n",
        "        # o = self.drop(self.activation(self.fc_layer_0(x)))\n",
        "\n",
        "        o = self.activation(self.fc_layer_0(x))\n",
        "        o = self.activation(self.fc_layer_1(o))\n",
        "        # o = self.activation(self.fc_layer_2(o))\n",
        "        \n",
        "        logit = self.output_layer(o)\n",
        "        rating = torch.sigmoid(logit)\n",
        "        return rating\n",
        "\n",
        "    def predict(self, feed_dict):\n",
        "        for key in feed_dict:\n",
        "            if type(feed_dict[key]) != type(None):\n",
        "                feed_dict[key] = torch.from_numpy(feed_dict[key]).to(dtype=torch.long, device=device)\n",
        "        output_scores = self.forward(feed_dict)\n",
        "        return output_scores.cpu().detach().numpy()\n",
        "\n",
        "# --> Testing the Network\n",
        "try:\n",
        "    del model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "model = RecSysNet(\n",
        "    n_users=num_users, \n",
        "    n_items=num_items, \n",
        ")    \n",
        "\n",
        "with torch.no_grad():\n",
        "    out = model(dict_loader)\n",
        "print(f'out forward shape: {out.shape}', end= '  --  ')\n",
        "print(f'out sample: {out[0]}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#========================================================================================\n",
        "deterministic()\n",
        "\n",
        "# --> Testing the Network\n",
        "try:\n",
        "    del model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "model = RecSysNet(\n",
        "    n_users=num_users, \n",
        "    n_items=num_items,\n",
        "    embedding_dim=128 \n",
        ").to(device)    \n",
        "\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters())\n",
        "\n",
        "TOPK = 10\n",
        "N_EPOCHS = 20\n",
        "\n",
        "BCE_loss_list, hr_list, ndcg_list, mrr_list = [], [], [], []\n",
        "\n",
        "# Check Init performance\n",
        "hr, ndcg, mrr = test(model, full_dataset, testRatings, topK=TOPK)\n",
        "print()\n",
        "hr_list.append(hr)\n",
        "ndcg_list.append(ndcg)\n",
        "mrr_list.append(mrr)\n",
        "BCE_loss_list.append(1)\n",
        "\n",
        "\n",
        "training_stats = []\n",
        "for epoch in range(1, N_EPOCHS+1):\n",
        "    loss_train = train_one_epoch(model, train_loader, loss_fn, optimizer, epoch, N_EPOCHS, device)\n",
        "    hr, ndcg, mrr = test(model,full_dataset, testRatings, topK=TOPK)\n",
        "    mrr_list.append(mrr)\n",
        "    hr_list.append(hr)\n",
        "    ndcg_list.append(ndcg)\n",
        "    BCE_loss_list.append(loss_train)\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch, \n",
        "            'Training Loss': loss_train, \n",
        "            'MRR': mrr, \n",
        "            'HR': hr,\n",
        "            'NDCG':ndcg,\n",
        "        }\n",
        "            )\n",
        "\n",
        "# plot_statistics(hr_list, ndcg_list, BCE_loss_list,model.get_alias(), \"./figs\")\n",
        "\n",
        "best_iter = np.argmax(np.array(mrr_list))\n",
        "best_mrr = mrr_list[best_iter]\n",
        "best_hr = hr_list[best_iter]\n",
        "best_ndcg = ndcg_list[best_iter]\n",
        "\n",
        "# Epoch = 10\n",
        "# Train Loss: 0.323103608017744\n",
        "# Eval: HR = 0.4899, NDCG = 0.2761 [0.9 s]\n",
        "\n",
        "print(f\"End. Best Iteration {best_iter}. MRR: {best_mrr :.4} -- HR: {best_hr :.4} -- NDCG: {best_ndcg :.4}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "out forward shape: torch.Size([256, 1])  --  out sample: tensor([0.4884])\n",
            "Experimento deterministico, seed: 2357 -- Existe 1 GPU Tesla P100-PCIE-16GB disponível.\n",
            "              Eval: MRR = 0.03428 -- HR = 0.1039 -- NDCG = 0.05004\n",
            "\n",
            "Epoch [1/20]: Train Loss: 0.3901\n",
            "              Eval: MRR = 0.1626 -- HR = 0.3955 -- NDCG = 0.2171\n",
            "Epoch [2/20]: Train Loss: 0.354\n",
            "              Eval: MRR = 0.1638 -- HR = 0.3977 -- NDCG = 0.2182\n",
            "Epoch [3/20]: Train Loss: 0.3451\n",
            "              Eval: MRR = 0.1736 -- HR = 0.4274 -- NDCG = 0.2323\n",
            "Epoch [4/20]: Train Loss: 0.3298\n",
            "              Eval: MRR = 0.1916 -- HR = 0.4836 -- NDCG = 0.2594\n",
            "Epoch [5/20]: Train Loss: 0.3055\n",
            "              Eval: MRR = 0.2227 -- HR = 0.5239 -- NDCG = 0.2928\n",
            "Epoch [6/20]: Train Loss: 0.2863\n",
            "              Eval: MRR = 0.2229 -- HR = 0.5249 -- NDCG = 0.2936\n",
            "Epoch [7/20]: Train Loss: 0.2721\n",
            "              Eval: MRR = 0.2344 -- HR = 0.5376 -- NDCG = 0.3056\n",
            "Epoch [8/20]: Train Loss: 0.2605\n",
            "              Eval: MRR = 0.2374 -- HR = 0.5461 -- NDCG = 0.31\n",
            "Epoch [9/20]: Train Loss: 0.2501\n",
            "              Eval: MRR = 0.2385 -- HR = 0.5578 -- NDCG = 0.3131\n",
            "Epoch [10/20]: Train Loss: 0.2399\n",
            "              Eval: MRR = 0.2261 -- HR = 0.5599 -- NDCG = 0.3042\n",
            "Epoch [11/20]: Train Loss: 0.2299\n",
            "              Eval: MRR = 0.2276 -- HR = 0.5631 -- NDCG = 0.3062\n",
            "Epoch [12/20]: Train Loss: 0.2198\n",
            "              Eval: MRR = 0.2299 -- HR = 0.5536 -- NDCG = 0.3059\n",
            "Epoch [13/20]: Train Loss: 0.2104\n",
            "              Eval: MRR = 0.2282 -- HR = 0.5376 -- NDCG = 0.3008\n",
            "Epoch [14/20]: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yq4Yi4VuoFBx"
      },
      "source": [
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "pd.set_option('precision', 2)\n",
        "df_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twDpehmg2Oiq"
      },
      "source": [
        "#Item Popularity Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AijHlIy2OTG"
      },
      "source": [
        "# Description: Implements the item popularity model for recommendations\n",
        "\n",
        "# Python imports\n",
        "import argparse\n",
        "from time import time\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Run ItemPop\")\n",
        "    parser.add_argument('--path', nargs='?', default='Data/',\n",
        "                        help='Input data path.')\n",
        "    parser.add_argument('--dataset', nargs='?', default='movielens',\n",
        "                        help='Choose a dataset.')\n",
        "    parser.add_argument('--num_neg_test', type=int, default=100,\n",
        "                        help='Number of negative instances to pair with a positive instance while testing')\n",
        "    \n",
        "    return parser.parse_args(args=[])\n",
        "\n",
        "\n",
        "class ItemPop():\n",
        "    def __init__(self, train_interaction_matrix: sp.dok_matrix):\n",
        "        \"\"\"\n",
        "        Simple popularity based recommender system\n",
        "        \"\"\"\n",
        "        self.__alias__ = \"Item Popularity without metadata\"\n",
        "        # Sum the occurences of each item to get is popularity, convert to array and \n",
        "        # lose the extra dimension\n",
        "        self.item_ratings = np.array(train_interaction_matrix.sum(axis=0, dtype=int)).flatten()\n",
        "\n",
        "    def forward(self):\n",
        "        pass\n",
        "\n",
        "    def predict(self, feeddict) -> np.array:\n",
        "        # returns the prediction score for each (user,item) pair in the input\n",
        "        items = feeddict['item_id']\n",
        "        output_scores = [self.item_ratings[itemid] for itemid in items]\n",
        "        return np.array(output_scores)\n",
        "\n",
        "    def get_alias(self):\n",
        "        return self.__alias__\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    path = args.path\n",
        "    dataset = args.dataset\n",
        "    num_negatives_test = args.num_neg_test\n",
        "    print(\"Model arguments: %s \" %(args))\n",
        "\n",
        "    topK = 10\n",
        "\n",
        "    # Load data\n",
        "\n",
        "    t1 = time()\n",
        "    full_dataset = MovieLensDataset(path + dataset, num_negatives_test= num_negatives_test)\n",
        "    train, testRatings, testNegatives = full_dataset.trainMatrix, full_dataset.testRatings, full_dataset.testNegatives\n",
        "    num_users, num_items = train.shape\n",
        "    print(\"Load data done [%.1f s]. #user=%d, #item=%d, #train=%d, #test=%d\"\n",
        "          % (time()-t1, num_users, num_items, train.nnz, len(testRatings)))\n",
        "\n",
        "    model = ItemPop(train)\n",
        "    test(model, full_dataset, topK)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzZ1sPH_ts6t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}